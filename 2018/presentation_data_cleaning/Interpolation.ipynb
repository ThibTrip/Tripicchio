{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description/notes\n",
    "This notebook is named interpolation because it makes it easy to remember what it does, that is to complete null values.\n",
    "But it does so without performing mathematical interpolation. Instead it looks at all the information given to find the best option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation_from_other_rows(key_series,\n",
    "                                  value_series,\n",
    "                                  disambiguate = True, \n",
    "                                  correct_potential_mistakes = False,\n",
    "                                  threshold_abs = 4,\n",
    "                                  threshold_pct = 0.7):\n",
    "    \"\"\"Completes null values in a series (value_series) by looking at all the rows from another series (key_series).\n",
    "    \n",
    "    If unique associations can be found e.g.: Paris -> France they are directly used to complete null values.\n",
    "    \n",
    "    In case of ambiguities and if the disambiguate parameter is True the most frequent value can be found using\n",
    "    threshold_abs (how many times a combination key-value must appear to be used) and threshold_pct (frequency in\n",
    "    relation to other combinations).\n",
    "    \n",
    "    In this case it is also possible to \"correct\" values not equal to the most frequent value found using threshold_abs\n",
    "    and threshold_pct. For this, set the correct_potential_mistakes parameter to True.\n",
    "    \n",
    "    Args:\n",
    "        key_series: pd.Series (the function has been tested only with string Series - dtype object)\n",
    "        value_series: pd.Series (the function has been tested only with string Series - dtype object)\n",
    "        disambiguate: bool (True or False)\n",
    "        correct_potential_mistakes: bool (True or False)\n",
    "        threshold_abs: integer\n",
    "        threshold_pct: float\n",
    "    \n",
    "    \n",
    "    Warning:\n",
    "        This function is meant for key_series and value_series which have unique or \"unique enough\" combinations.\n",
    "        It is recommanded to do use it in combination with other methods for completing missing values (e.g. using\n",
    "        databases like Geonames) and do a thorough verification of the results.\n",
    "        Some examples could be:\n",
    "            - first name -> gender \n",
    "            - region -> country\n",
    "            - client -> person responsible for a client\n",
    "            - company name -> company type\n",
    "    \n",
    "    Returns:\n",
    "        np.array of completed values\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Verifies arguments validity \n",
    "    if threshold_abs < 1:\n",
    "        raise ValueError('Minimal accepted value for threshold_abs is 1')\n",
    "    \n",
    "    if not isinstance(threshold_abs,int):\n",
    "        raise TypeError('threshold_abs should be of type int')\n",
    "    \n",
    "    if threshold_pct <= 0.5:\n",
    "        raise ValueError('Minimal accepted value for threshold_pct is strictly above 0.5')\n",
    "        \n",
    "    \n",
    "    # Creates a DataFrame from key and series and a copy of this df for later\n",
    "    df_key_value = pd.DataFrame({'Key':key_series,'Value':value_series})\n",
    "    df_copy = df_key_value.copy(deep = True)\n",
    "\n",
    "    # Drops null lines and reset index\n",
    "    df_key_value.dropna(axis = 0,how = 'any',inplace = True)\n",
    "    df_key_value.reset_index(drop = True,inplace = True)\n",
    "\n",
    "    # Tuple-izes key and values series\n",
    "    combinations = pd.Series(df_key_value.itertuples(index = False))\n",
    "\n",
    "    # Transforms tuple-ized key and values series to a DataFrame in order to join it to the previously created df\n",
    "    combinations.name = 'Combination'\n",
    "    df_combinations = pd.DataFrame(combinations)\n",
    "    df_combinations = df_combinations.join(df_key_value)\n",
    "\n",
    "    # Counts unique combinations per key then maps it to a column   \n",
    "    unique_comb_per_key = df_combinations.groupby('Key')['Combination'].nunique()\n",
    "    df_combinations['Count unique Combination per key'] = df_combinations['Key'].map(unique_comb_per_key)\n",
    "\n",
    "    # Counts the frequency of the combinations then maps it to a column\n",
    "    count_comb_total = df_combinations['Combination'].value_counts()\n",
    "    df_combinations['Count Combination'] = df_combinations['Combination'].map(count_comb_total)\n",
    "    df_combinations = df_combinations.drop_duplicates()\n",
    "\n",
    "    # Counts the number of all combinations for each key\n",
    "    count_all_comb_per_key = df_combinations.groupby(['Key'])['Count Combination'].sum(axis = 0)\n",
    "    df_combinations['Count all combinations per key'] = df_combinations['Key'].map(count_all_comb_per_key)\n",
    "\n",
    "    # Divides to find percentage\n",
    "    df_combinations['Count Combination (Pct)'] = df_combinations['Count Combination']/df_combinations['Count all combinations per key']\n",
    "\n",
    "    # Gets all mappings that fit requirements\n",
    "    ## (Above percentage threshold (default more than 70% of the values) \n",
    "    ## AND above absolute threshold (default more than 4 occurences)) \n",
    "    ## OR (Count unique == 1 ) which means there is only one unique association such as Paris -> France.\n",
    "    if disambiguate == True:\n",
    "        df_combinations = df_combinations[((df_combinations['Count Combination (Pct)'] >= threshold_pct) & \n",
    "                                          (df_combinations['Count Combination'] >= threshold_abs)) | \n",
    "                                          (df_combinations['Count unique Combination per key'] == 1)]\n",
    "\n",
    "    else:\n",
    "        df_combinations = df_combinations[df_combinations['Count unique Combination per key'] == 1]\n",
    "\n",
    "\n",
    "    # Merges the new values\n",
    "    df_combinations = df_combinations.rename(columns = {'Value':'New values'})\n",
    "    df_copy = df_copy.merge(df_combinations[['Key','New values']],on = ['Key'], how = 'left')\n",
    "\n",
    "    # Depending on the arguments configuration some values can get lost in the way, we retrieve them here\n",
    "    df_copy.loc[(df_copy['Value'].notna()) &\n",
    "                (df_copy['New values'].isna()),'New values'] = df_copy['Value']\n",
    "\n",
    "    # What to do if the most frequent value is not in accordance with some rows\n",
    "    ## Let the values as they are\n",
    "    if correct_potential_mistakes == False:\n",
    "        # If old != new and both aren't null take the old value back\n",
    "        df_copy.loc[(df_copy['New values'] != df_copy['Value']) & \n",
    "                    (pd.notna(df_copy[['Value','New values']]).all(axis = 1)),'New values'] = df_copy['Value']\n",
    "\n",
    "    ## Else no need to do anything the new values can be returned as they are\n",
    "\n",
    "    return df_copy['New values'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "Uncomment the last line to see the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class interpolation_example(): # class used only for the namespace\n",
    "    \n",
    "    # creating data\n",
    "    dates = [datetime.date(2018,1,1)+datetime.timedelta(i) for i in range(50)]\n",
    "    items_id = np.arange(1,51)\n",
    "    cities = ['Firenze','London','London','Marseille','Marseille']\n",
    "    cities += ['Paris' for i in range(21)]\n",
    "    cities += ['Firenze' for i in range(24)]\n",
    "    \n",
    "    countries = ['Deutschland','Großbritannien',np.nan,'Frankreich','Frankreich']\n",
    "    countries += np.random.choice(['Frankreich','Großbritannien',np.nan],21, p = [0.8,0.1,0.1]).tolist()\n",
    "    countries += np.random.choice(['Italien',np.nan],24, p = [0.4,0.6]).tolist()\n",
    "\n",
    "    data = {'Purchase_date': dates,'Item_ID':items_id,'City':cities,'Country':countries}\n",
    "    \n",
    "    # Creating the DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Replacing \"nan\" string with true nulls (this happened because numpy casted np.nan to as tring with np.random.choice)\n",
    "    df.replace('nan',np.nan,inplace = True)\n",
    "\n",
    "    case1 = 'Country guessed if unequivocal'\n",
    "    case2 = 'Country guessed with statistics'\n",
    "    case3 = 'Country guessed with statistics + correction'\n",
    "\n",
    "\n",
    "\n",
    "    # case1 : one unique associated country found for a city -> complete null values by this country\n",
    "    df[case1] = interpolation_from_other_rows(key_series = df['City'],\n",
    "                                              value_series = df['Country'],\n",
    "                                              disambiguate = False)\n",
    "\n",
    "    # case2 : more than one associated country found for the same city -> find most frequent value\n",
    "    df[case2] = interpolation_from_other_rows(key_series = df['City'],\n",
    "                                              value_series = df['Country'],\n",
    "                                              disambiguate = True,\n",
    "                                              correct_potential_mistakes = False)\n",
    "\n",
    "    # case3 : more than one associated country found for the same city -> find most frequent value and \"correct\" values\n",
    "    # not equal to the most frequent value\n",
    "    df[case3] = interpolation_from_other_rows(key_series = df['City'],\n",
    "                                              value_series = df['Country'],\n",
    "                                              disambiguate = True,\n",
    "                                              correct_potential_mistakes = True)\n",
    "    results = df\n",
    "    \n",
    "#interpolation_example.results"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
